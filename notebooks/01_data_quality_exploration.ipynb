{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Exploration for CRISP Pipeline\n",
    "\n",
    "This notebook helps you understand and assess your OMOP CDM data quality before running the CRISP pipeline.\n",
    "\n",
    "**Purpose:**\n",
    "- Examine data completeness and quality\n",
    "- Identify potential issues before processing\n",
    "- Generate quality metrics and visualizations\n",
    "\n",
    "**Expected runtime:** ~5 minutes for 1000 patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OMOP data tables\n",
    "data_dir = Path('../data')\n",
    "\n",
    "# Define required tables\n",
    "required_tables = [\n",
    "    'PERSON', 'VISIT_OCCURRENCE', 'CONDITION_OCCURRENCE',\n",
    "    'PROCEDURE_OCCURRENCE', 'DRUG_EXPOSURE', 'MEASUREMENT',\n",
    "    'OBSERVATION'\n",
    "]\n",
    "\n",
    "# Load available tables\n",
    "tables = {}\n",
    "missing_tables = []\n",
    "\n",
    "for table_name in required_tables:\n",
    "    file_path = data_dir / f\"{table_name}.csv\"\n",
    "    if file_path.exists():\n",
    "        print(f\"Loading {table_name}...\", end=' ')\n",
    "        tables[table_name] = pd.read_csv(file_path)\n",
    "        print(f\"âœ“ ({len(tables[table_name]):,} rows)\")\n",
    "    else:\n",
    "        missing_tables.append(table_name)\n",
    "        print(f\"âš ï¸  {table_name} not found\")\n",
    "\n",
    "if missing_tables:\n",
    "    print(f\"\\nâš ï¸  Missing {len(missing_tables)} tables: {missing_tables}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All {len(required_tables)} required tables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table sizes and basic statistics\n",
    "table_stats = []\n",
    "\n",
    "for name, df in tables.items():\n",
    "    stats = {\n",
    "        'Table': name,\n",
    "        'Rows': len(df),\n",
    "        'Columns': len(df.columns),\n",
    "        'Memory (MB)': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'Null %': (df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100) if len(df) > 0 else 0\n",
    "    }\n",
    "    table_stats.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(table_stats).round(2)\n",
    "stats_df = stats_df.sort_values('Rows', ascending=False)\n",
    "\n",
    "print(\"ðŸ“Š Table Statistics Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(stats_df.to_string(index=False))\n",
    "print(f\"\\nTotal rows across all tables: {stats_df['Rows'].sum():,}\")\n",
    "print(f\"Total memory usage: {stats_df['Memory (MB)'].sum():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in critical columns\n",
    "print(\"ðŸ” Missing Values in Critical Columns:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "critical_columns = {\n",
    "    'PERSON': ['person_id', 'gender_concept_id', 'year_of_birth'],\n",
    "    'VISIT_OCCURRENCE': ['visit_occurrence_id', 'person_id', 'visit_start_date'],\n",
    "    'MEASUREMENT': ['measurement_id', 'person_id', 'measurement_concept_id'],\n",
    "    'CONDITION_OCCURRENCE': ['condition_occurrence_id', 'person_id', 'condition_concept_id']\n",
    "}\n",
    "\n",
    "for table_name, columns in critical_columns.items():\n",
    "    if table_name in tables:\n",
    "        df = tables[table_name]\n",
    "        print(f\"\\n{table_name}:\")\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                null_count = df[col].isnull().sum()\n",
    "                null_pct = (null_count / len(df) * 100) if len(df) > 0 else 0\n",
    "                if null_count > 0:\n",
    "                    print(f\"  âš ï¸  {col}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "                else:\n",
    "                    print(f\"  âœ“ {col}: no nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate person_ids\n",
    "if 'PERSON' in tables:\n",
    "    person_df = tables['PERSON']\n",
    "    total_persons = len(person_df)\n",
    "    unique_persons = person_df['person_id'].nunique()\n",
    "    duplicates = total_persons - unique_persons\n",
    "    \n",
    "    print(\"ðŸ‘¥ Person ID Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total person records: {total_persons:,}\")\n",
    "    print(f\"Unique person IDs: {unique_persons:,}\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        print(f\"âš ï¸  Found {duplicates} duplicate person_id entries\")\n",
    "        # Show duplicate IDs\n",
    "        dup_ids = person_df[person_df.duplicated('person_id', keep=False)]['person_id'].value_counts().head()\n",
    "        print(\"\\nTop duplicate person_ids:\")\n",
    "        print(dup_ids)\n",
    "    else:\n",
    "        print(\"âœ… No duplicate person_ids found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range analysis\n",
    "print(\"ðŸ“… Temporal Coverage Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "date_columns = {\n",
    "    'VISIT_OCCURRENCE': ['visit_start_date', 'visit_end_date'],\n",
    "    'CONDITION_OCCURRENCE': ['condition_start_date'],\n",
    "    'MEASUREMENT': ['measurement_date'],\n",
    "    'DRUG_EXPOSURE': ['drug_exposure_start_date']\n",
    "}\n",
    "\n",
    "for table_name, cols in date_columns.items():\n",
    "    if table_name in tables:\n",
    "        df = tables[table_name]\n",
    "        print(f\"\\n{table_name}:\")\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                # Convert to datetime\n",
    "                dates = pd.to_datetime(df[col], errors='coerce')\n",
    "                valid_dates = dates.dropna()\n",
    "                \n",
    "                if len(valid_dates) > 0:\n",
    "                    min_date = valid_dates.min()\n",
    "                    max_date = valid_dates.max()\n",
    "                    span_years = (max_date - min_date).days / 365.25\n",
    "                    \n",
    "                    print(f\"  {col}:\")\n",
    "                    print(f\"    Range: {min_date.date()} to {max_date.date()} ({span_years:.1f} years)\")\n",
    "                    print(f\"    Invalid dates: {len(dates) - len(valid_dates):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign key validation - Check if all persons in other tables exist in PERSON\n",
    "if 'PERSON' in tables:\n",
    "    person_ids = set(tables['PERSON']['person_id'].unique())\n",
    "    \n",
    "    print(\"ðŸ”— Foreign Key Validation (person_id):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for table_name, df in tables.items():\n",
    "        if table_name != 'PERSON' and 'person_id' in df.columns:\n",
    "            table_person_ids = set(df['person_id'].unique())\n",
    "            orphaned = table_person_ids - person_ids\n",
    "            \n",
    "            if orphaned:\n",
    "                print(f\"âš ï¸  {table_name}: {len(orphaned)} person_ids not found in PERSON table\")\n",
    "            else:\n",
    "                print(f\"âœ“ {table_name}: All person_ids valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Quality Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Calculate missing percentages for each table\n",
    "missing_data = {}\n",
    "for name, df in tables.items():\n",
    "    if len(df) > 0:\n",
    "        missing_pct = (df.isnull().sum() / len(df) * 100).round(1)\n",
    "        # Only show columns with any missing data\n",
    "        missing_cols = missing_pct[missing_pct > 0]\n",
    "        if len(missing_cols) > 0:\n",
    "            missing_data[name] = missing_cols\n",
    "\n",
    "if missing_data:\n",
    "    # Create a matrix for heatmap\n",
    "    all_cols = set()\n",
    "    for cols in missing_data.values():\n",
    "        all_cols.update(cols.index)\n",
    "    \n",
    "    matrix_data = []\n",
    "    table_names = []\n",
    "    \n",
    "    for table_name, missing_cols in missing_data.items():\n",
    "        row = [missing_cols.get(col, 0) for col in sorted(all_cols)]\n",
    "        matrix_data.append(row)\n",
    "        table_names.append(table_name)\n",
    "    \n",
    "    if matrix_data:\n",
    "        sns.heatmap(matrix_data, \n",
    "                   xticklabels=sorted(all_cols),\n",
    "                   yticklabels=table_names,\n",
    "                   annot=True, fmt='.1f',\n",
    "                   cmap='YlOrRd',\n",
    "                   cbar_kws={'label': 'Missing %'},\n",
    "                   ax=ax)\n",
    "        plt.title('Missing Data Heatmap (% missing by column)', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"âœ… No missing data found in any table!\")\n",
    "else:\n",
    "    print(\"âœ… No missing data found in any table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record count bar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sort tables by row count\n",
    "sorted_stats = stats_df.sort_values('Rows', ascending=True)\n",
    "\n",
    "# Bar chart of row counts\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(sorted_stats)))\n",
    "bars = ax1.barh(sorted_stats['Table'], sorted_stats['Rows'], color=colors)\n",
    "ax1.set_xlabel('Number of Records', fontsize=12)\n",
    "ax1.set_title('Record Count by Table', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, sorted_stats['Rows']):\n",
    "    ax1.text(value, bar.get_y() + bar.get_height()/2, f'{value:,}', \n",
    "            ha='left', va='center', fontsize=10)\n",
    "\n",
    "# Pie chart of memory usage\n",
    "memory_data = sorted_stats[sorted_stats['Memory (MB)'] > 0]\n",
    "if len(memory_data) > 0:\n",
    "    ax2.pie(memory_data['Memory (MB)'], \n",
    "           labels=memory_data['Table'],\n",
    "           autopct='%1.1f%%',\n",
    "           startangle=90)\n",
    "    ax2.set_title('Memory Usage Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline visualization - data coverage over time\n",
    "if 'VISIT_OCCURRENCE' in tables and len(tables['VISIT_OCCURRENCE']) > 0:\n",
    "    visit_df = tables['VISIT_OCCURRENCE'].copy()\n",
    "    \n",
    "    # Convert dates\n",
    "    visit_df['visit_start_date'] = pd.to_datetime(visit_df['visit_start_date'], errors='coerce')\n",
    "    visit_df = visit_df.dropna(subset=['visit_start_date'])\n",
    "    \n",
    "    if len(visit_df) > 0:\n",
    "        # Group by month\n",
    "        visit_df['year_month'] = visit_df['visit_start_date'].dt.to_period('M')\n",
    "        monthly_visits = visit_df.groupby('year_month').size()\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(14, 5))\n",
    "        monthly_visits.plot(kind='line', ax=ax, linewidth=2, marker='o', markersize=4)\n",
    "        ax.set_xlabel('Time Period', fontsize=12)\n",
    "        ax.set_ylabel('Number of Visits', fontsize=12)\n",
    "        ax.set_title('Temporal Data Coverage (Visits Over Time)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        avg_visits = monthly_visits.mean()\n",
    "        ax.axhline(y=avg_visits, color='r', linestyle='--', alpha=0.5, label=f'Average: {avg_visits:.0f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"ðŸ“Š Temporal Coverage Summary:\")\n",
    "        print(f\"  â€¢ Date range: {monthly_visits.index.min()} to {monthly_visits.index.max()}\")\n",
    "        print(f\"  â€¢ Total months: {len(monthly_visits)}\")\n",
    "        print(f\"  â€¢ Average visits/month: {avg_visits:.0f}\")\n",
    "        print(f\"  â€¢ Peak month: {monthly_visits.idxmax()} ({monthly_visits.max():,} visits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Quality Report Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive quality report\n",
    "print(\"ðŸ“‹ DATA QUALITY REPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect all quality metrics\n",
    "quality_metrics = {\n",
    "    'Total Tables Loaded': len(tables),\n",
    "    'Missing Tables': len(missing_tables),\n",
    "    'Total Records': stats_df['Rows'].sum(),\n",
    "    'Total Unique Patients': tables['PERSON']['person_id'].nunique() if 'PERSON' in tables else 0,\n",
    "    'Data Size (MB)': stats_df['Memory (MB)'].sum(),\n",
    "    'Tables with Missing Data': sum(1 for name, df in tables.items() if df.isnull().any().any()),\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "for metric, value in quality_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric:.<30} {value:,.1f}\")\n",
    "    else:\n",
    "        print(f\"{metric:.<30} {value:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify critical issues and provide recommendations\n",
    "print(\"âš ï¸  CRITICAL ISSUES & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "issues = []\n",
    "recommendations = []\n",
    "\n",
    "# Check for critical issues\n",
    "if missing_tables:\n",
    "    issues.append(f\"Missing {len(missing_tables)} required tables: {', '.join(missing_tables)}\")\n",
    "    recommendations.append(\"Ensure all required OMOP tables are present before running pipeline\")\n",
    "\n",
    "if 'PERSON' in tables:\n",
    "    person_df = tables['PERSON']\n",
    "    if len(person_df) == 0:\n",
    "        issues.append(\"PERSON table is empty\")\n",
    "        recommendations.append(\"Load patient data before proceeding\")\n",
    "    elif person_df['person_id'].duplicated().any():\n",
    "        issues.append(\"Duplicate person_ids found\")\n",
    "        recommendations.append(\"Review and deduplicate PERSON table\")\n",
    "\n",
    "# Check for excessive missing data\n",
    "for name, df in tables.items():\n",
    "    null_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100) if len(df) > 0 else 0\n",
    "    if null_pct > 50:\n",
    "        issues.append(f\"{name} has {null_pct:.1f}% missing data\")\n",
    "        recommendations.append(f\"Review data quality for {name} table\")\n",
    "\n",
    "# Display results\n",
    "if issues:\n",
    "    print(\"\\nðŸ”´ Issues Found:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Recommendations:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No critical issues found!\")\n",
    "    print(\"\\nYour data appears ready for the CRISP pipeline.\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Run data validation: python data_preparation/validate_data.py\")\n",
    "    print(\"  2. Execute pipeline: python pipeline_modules/run_all_module.py\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Report generated successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
